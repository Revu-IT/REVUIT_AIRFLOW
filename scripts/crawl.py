from google_play_scraper import reviews, Sort
import os
import time
import pandas as pd
import numpy as np
from datetime import datetime

COMPANY_NAME = os.getenv("COMPANY_NAME", "gmarket").lower()
APP_ID = os.getenv("APP_ID", "com.ebay.kr.gmarket")
DATA_FOLDER = "/opt/airflow/data"
file_path = os.path.join(DATA_FOLDER, f"{COMPANY_NAME}_review_result.csv")
BATCH_SIZE = 200
TARGET_COUNT = 30_500

# ê¸°ì¡´ íŒŒì¼ ë¡œë“œ ë° ë‚ ì§œ íŒŒì‹±
if os.path.exists(file_path):
    df_old = pd.read_csv(file_path, encoding='utf-8-sig')

    if 'date' in df_old.columns:
        df_old['date'] = pd.to_datetime(df_old['date'], errors='coerce')
        valid_dates = df_old['date'].dropna()

        if not valid_dates.empty:
            start_date = valid_dates.max()
        else:
            print("ğŸš« ìœ íš¨í•œ ë‚ ì§œ ì—†ìŒ. ê¸°ë³¸ê°’ ì‚¬ìš©")
            start_date = datetime(2023, 1, 1)
    else:
        print("ğŸš« date ì»¬ëŸ¼ ì—†ìŒ. ê¸°ë³¸ê°’ ì‚¬ìš©")
        start_date = datetime(2023, 1, 1)
        df_old['date'] = np.nan
else:
    start_date = datetime(2023, 1, 1)
    df_old = pd.DataFrame(columns=['date'])

print(f"âœ… í¬ë¡¤ë§ ì‹œì‘ ê¸°ì¤€ì¼: {start_date}")

# í¬ë¡¤ë§ ì‹œì‘
all_reviews = []
token = None

while len(all_reviews) < TARGET_COUNT:
    result, token = reviews(
        APP_ID,
        lang='ko',
        country='kr',
        sort=Sort.NEWEST,
        count=BATCH_SIZE,
        continuation_token=token
    )

    # ê¸°ì¤€ì¼ ì´í›„ ë¦¬ë·°ë§Œ í•„í„°ë§
    filtered = [r for r in result if r.get('at') and r['at'] > start_date]

    if not filtered:
        break

    all_reviews.extend(filtered)
    print(f"âœ… {len(all_reviews)} / {TARGET_COUNT} ë¦¬ë·° ìˆ˜ì§‘ë¨")

    if token is None:
        break

    time.sleep(3)

# ê²°ê³¼ ì €ì¥
if not all_reviews:
    print("ğŸš« ìƒˆë¡œìš´ ë¦¬ë·° ì—†ìŒ. ì¢…ë£Œ")
else:
    df_new = pd.DataFrame(all_reviews)
    df_new = df_new.rename(columns={
        'score': 'score',
        'at': 'date',
        'content': 'content',
        'thumbsUpCount': 'like',
    })
    df_new = df_new[['score', 'date', 'content', 'like']]
    df_new['cleaned_text'] = np.nan
    df_new['positive'] = np.nan

    df_new['date'] = pd.to_datetime(df_new['date'], errors='coerce')
    df_old['date'] = pd.to_datetime(df_old['date'], errors='coerce')

    print(f"ğŸ•’ ìƒˆë¡œ ìˆ˜ì§‘í•œ ë¦¬ë·° ì¤‘ ê°€ì¥ ìµœì‹  ë‚ ì§œ: {df_new['date'].max()}")

    df_all = pd.concat([df_old, df_new], ignore_index=True)
    df_all.drop_duplicates(subset=['content', 'date'], keep='last', inplace=True)
    df_all = df_all.sort_values(by='date', ascending=False)
    df_all.to_csv(file_path, index=False, encoding='utf-8-sig')
    print(f"âœ… ìƒˆ ë¦¬ë·° {len(df_new)}ê°œ ì¶”ê°€ ì €ì¥ ì™„ë£Œ: {file_path}")