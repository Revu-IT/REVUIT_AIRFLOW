from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime
import os
import sys
import importlib.util
from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.http import MediaFileUpload

# ì—ì–´í”Œë¡œìš° í™˜ê²½ ê²½ë¡œ ì„¤ì •
AIRFLOW_HOME = "/opt/airflow"
DAGS_FOLDER = os.path.join(AIRFLOW_HOME, "dags")

# ê° í´ë” ê²½ë¡œ ì„¤ì •
SCRIPTS_FOLDER = os.path.join(AIRFLOW_HOME, "scripts")
DATA_FOLDER = os.path.join(AIRFLOW_HOME, "data")
KEYS_FOLDER = os.path.join(AIRFLOW_HOME, "keys")

# ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ ê²½ë¡œ
CRAWL_SCRIPT = os.path.join(SCRIPTS_FOLDER, "crawl.py")
OKT_SCRIPT = os.path.join(SCRIPTS_FOLDER, "okt.py")
SENTIMENT_SCRIPT = os.path.join(SCRIPTS_FOLDER, "sentiment.py")
DEPARTMENT_SCRIPT = os.path.join(SCRIPTS_FOLDER, "department.py")

# ì„œë¹„ìŠ¤ ê³„ì • íŒŒì¼ ê²½ë¡œ
SERVICE_ACCOUNT_FILE = os.path.join(KEYS_FOLDER, "airflow-463709-f8a4c39f2f87.json") # âš ï¸ ê°ìž ì„œë¹„ìŠ¤ ê³„ì • íŒŒì¼ëª…ìœ¼ë¡œ ë³€ê²½

# ìŠ¤í¬ë¦½íŠ¸ ëª¨ë“ˆ ë¡œë“œ ë° ì‹¤í–‰ í•¨ìˆ˜
def execute_python_script(script_path):
    print(f"ìŠ¤í¬ë¦½íŠ¸ ê²½ë¡œ: {script_path}")
    
    # ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ ì¡´ìž¬ í™•ì¸
    if not os.path.exists(script_path):
        raise FileNotFoundError(f"ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {script_path}")
    
    # ìŠ¤í¬ë¦½íŠ¸ ì´ë¦„ ì¶”ì¶œ
    script_name = os.path.basename(script_path)
    module_name = os.path.splitext(script_name)[0]
    
    # ëª¨ë“ˆ ìŠ¤íŽ™ ìƒì„±
    spec = importlib.util.spec_from_file_location(module_name, script_path)
    if spec is None:
        raise ImportError(f"ìŠ¤í¬ë¦½íŠ¸ë¥¼ ëª¨ë“ˆë¡œ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {script_path}")
    
    # ëª¨ë“ˆ ìƒì„±
    module = importlib.util.module_from_spec(spec)
    
    # ìŠ¤í¬ë¦½íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ ì‹œìŠ¤í…œ ê²½ë¡œì— ì¶”ê°€
    script_dir = os.path.dirname(script_path)
    if script_dir not in sys.path:
        sys.path.insert(0, script_dir)
    
    # í˜„ìž¬ ìž‘ì—… ë””ë ‰í† ë¦¬ ë³€ê²½
    original_dir = os.getcwd()
    os.chdir(script_dir)
    
    try:
        # ëª¨ë“ˆ ì‹¤í–‰
        spec.loader.exec_module(module)
        print(f"ìŠ¤í¬ë¦½íŠ¸ {script_name} ì‹¤í–‰ ì™„ë£Œ")
    finally:
        # ì›ëž˜ ë””ë ‰í† ë¦¬ë¡œ ë³µì›
        os.chdir(original_dir)

# ê° ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ í•¨ìˆ˜
def run_crawling_script():
    print(f"í¬ë¡¤ë§ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì¤‘... ê²½ë¡œ: {CRAWL_SCRIPT}")
    execute_python_script(CRAWL_SCRIPT)

def run_general_preprocessing():
    print(f"ì¼ë°˜ ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì¤‘... ê²½ë¡œ: {OKT_SCRIPT}")
    execute_python_script(OKT_SCRIPT)

def run_sentiment_preprocessing():
    print(f"ê°ì • ë¶„ì„ ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì¤‘... ê²½ë¡œ: {SENTIMENT_SCRIPT}")
    execute_python_script(SENTIMENT_SCRIPT)
    
def run_department_classification():
    print(f"ë¶€ì„œ ë¶„ë¥˜ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì¤‘... ê²½ë¡œ: {DEPARTMENT_SCRIPT}")
    execute_python_script(DEPARTMENT_SCRIPT)

# êµ¬ê¸€ ë“œë¼ì´ë¸Œ ì—…ë¡œë“œ
def upload_results_to_drive():
    print(f"ë°ì´í„° í´ë” ê²½ë¡œ: {DATA_FOLDER}")
    print(f"ì„œë¹„ìŠ¤ ê³„ì • íŒŒì¼ ê²½ë¡œ: {SERVICE_ACCOUNT_FILE}")
    
    # ì„œë¹„ìŠ¤ ê³„ì • íŒŒì¼ í™•ì¸
    if not os.path.exists(SERVICE_ACCOUNT_FILE):
        raise FileNotFoundError(f"ì„œë¹„ìŠ¤ ê³„ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {SERVICE_ACCOUNT_FILE}")
    
    # ì—…ë¡œë“œí•  íŒŒì¼ ê²½ë¡œ (âš ï¸ ê°ìž ì´ì»¤ë¨¸ìŠ¤ëª…ìœ¼ë¡œ ë³€ê²½)
    result_file = os.path.join(DATA_FOLDER, "G_review_result.csv")
    if not os.path.exists(result_file):
        raise FileNotFoundError(f"G_review_result.csv íŒŒì¼ì´ ì¡´ìž¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {result_file}")
    
    # êµ¬ê¸€ ë“œë¼ì´ë¸Œ ì¸ì¦
    credentials = service_account.Credentials.from_service_account_file(
        SERVICE_ACCOUNT_FILE,
        scopes=["https://www.googleapis.com/auth/drive"]
    )
    
    service = build('drive', 'v3', credentials=credentials)
    
    # ë“œë¼ì´ë¸Œ í´ë” ID
    folder_id = "1FBcOCEqnQ6NtLNgrxgwXb0LtSaBmsBoh"
    
    # ê¸°ì¡´ ê°™ì€ ì´ë¦„ì˜ íŒŒì¼ ì‚­ì œ
    print("ðŸ” ê¸°ì¡´ ë™ì¼ ì´ë¦„ íŒŒì¼ í™•ì¸ ì¤‘...")
    query = f"'{folder_id}' in parents and name = 'G_review_result.csv' and trashed = false" # âš ï¸ ê°ìž ì´ì»¤ë¨¸ìŠ¤ëª…ìœ¼ë¡œ ë³€ê²½
    response = service.files().list(q=query, spaces='drive', fields='files(id, name)').execute()
    for file in response.get('files', []):
        service.files().delete(fileId=file['id']).execute()
        print(f"ðŸ—‘ï¸ ê¸°ì¡´ íŒŒì¼ ì‚­ì œë¨: {file['name']} (ID: {file['id']})")
    
    # ìƒˆ íŒŒì¼ ì—…ë¡œë“œ (âš ï¸ ê°ìž ì´ì»¤ë¨¸ìŠ¤ëª…ìœ¼ë¡œ ë³€ê²½)
    file_metadata = {
        'name': 'G_review_result.csv',
        'parents': [folder_id]
    }
    
    media = MediaFileUpload(result_file, resumable=True)
    uploaded_file = service.files().create(
        body=file_metadata,
        media_body=media,
        fields='id',
        supportsAllDrives=True
    ).execute()
    
    print(f"âœ… G_review_result.csv ì—…ë¡œë“œ ì™„ë£Œ, ID: {uploaded_file.get('id')}")

# DAG ì •ì˜
with DAG(
    dag_id="reviewit_pipeline_controller",
    schedule_interval="0 9 * * *",  # ë§¤ì¼ ì•„ì¹¨ 9ì‹œ
    start_date=datetime(2023, 1, 1),
    catchup=False,
    tags=["pipeline"]
) as dag:
    
    crawling = PythonOperator(
        task_id="run_crawling_script",
        python_callable=run_crawling_script
    )
    
    general_preprocessing = PythonOperator(
        task_id="run_general_preprocessing",
        python_callable=run_general_preprocessing
    )
    
    sentiment_preprocessing = PythonOperator(
        task_id="run_sentiment_preprocessing",
        python_callable=run_sentiment_preprocessing
    )
    
    department_classification = PythonOperator(
        task_id="run_department_classification",
        python_callable=run_department_classification
    )
    
    upload_to_drive = PythonOperator(
        task_id="upload_results_to_drive",
        python_callable=upload_results_to_drive
    )
    
    # íƒœìŠ¤í¬ ì˜ì¡´ì„± ì„¤ì •
    crawling >> general_preprocessing >> sentiment_preprocessing >> department_classification >> upload_to_drive